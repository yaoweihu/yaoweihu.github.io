---
layout: distill
title: Position Embeddings
description: In this blog, I'm gonna introduce some basic concepts about position embeddings, including absolute, relative and rotary position embeddings.
giscus_comments: true
date: 2025-12-24

authors:
  - name: Yaowei Hu
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: Walmart Inc.
  
bibliography:

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: 高并发编程
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: 
  - name: 
  
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >

---
Transformers are foundational architecture of LLMs. However, they are **position-agnostic**. Therefore, positional information must be explicitly provided to help the model better understand word order in the sequence.

## Absolute Position Embedding

## Relative Position Embedding