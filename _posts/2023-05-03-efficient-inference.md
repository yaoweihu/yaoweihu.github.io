---
layout: distill
title: Efficient Inference
description: Model Compression, Efficient Inference
giscus_comments: true
date: 2023-05-03

authors:
  - name: Yaowei Hu
    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name:
  
bibliography:

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: 
  - name: 
  
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >

---

## Introduction
In existing research, efficient inference (also known as model compression) can be achieved by following several methods: 
- network pruning & sparse neural network
- quantization
- neural architecture search
- knowledge distillation

Efficient training:
- gradient compression
- on-device training
- federated learning

